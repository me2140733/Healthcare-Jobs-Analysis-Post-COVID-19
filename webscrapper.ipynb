{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library used to query a website\n",
    "import urllib3\n",
    "#import the Beautiful soup functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "'Accept-Encoding': 'none',\n",
    "'Accept-Language': 'en-US,en;q=0.8',\n",
    "'Connection': 'keep-alive'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first step to generate links\n",
    "alllinks=[]\n",
    "alllinksJOBS=\"\"\n",
    "finallinks = []\n",
    "\n",
    "numberofpages = 20 #THIS IS IMPORTANT HOW MANY PAGES TO DOWNLOAD\n",
    "for x in range(numberofpages):\n",
    "    alllinks.append(\"https://www.naukrigulf.com/healthcare-jobs-in-uae-%d\" %x)\n",
    "#alllinks.append(\"https://www.naukrigulf.com/oil-and-gas-jobs-%d?industry=36\" % x)\n",
    "\n",
    "#iterate over all links\n",
    "for links in alllinks:\n",
    "    req = urllib.request.Request(links, headers=hdr)\n",
    "    page = urllib.request.urlopen(req)\n",
    "    naukrigulfHtml = page.read()\n",
    "    page.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(naukrigulfHtml, \"html.parser\")  # Parse the html in the 'page' variable, and store it in Beautiful Soup format\n",
    "NaukriAllLinks = soup.find_all(\"a\")  # we find only the links which start by \"a\" HTML code\n",
    "\n",
    "for links in NaukriAllLinks:\n",
    "    tmplink = links.get('href')  # inside all links with \"a\" we want only those with \"href\" HTML code\n",
    "    alllinksJOBS = alllinksJOBS + \"\\n\" + str(tmplink)  # make one big string will all links\n",
    "\n",
    "text_file = open(\"alllinksNAUKRI.txt\", \"w\")  # store it in txt file for further processing\n",
    "text_file.write(alllinksJOBS)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 Jobs to download:\n"
     ]
    }
   ],
   "source": [
    "openfile = open(\"alllinksNAUKRI.txt\")\n",
    "for line in openfile:\n",
    "# if re.search(r\"^/pagead/clk.*\", line, re.MULTILINE): #using regular expression we take only the valid links for jobs\n",
    "#     finallinks.append(line.rstrip()) #store them in list for further processing (to remove duplicate links)\n",
    "    if re.search(r\"^https://www.naukrigulf.com/.*\\d{12}$\", line, flags=re.MULTILINE):  # using regular expression we take only the valid links for jobs\n",
    "        finallinks.append(line.rstrip()) #store them in list for further processing (to remove duplicate links)\n",
    "        #openfile.close()\n",
    "\n",
    "#here are the JOB links from all INDEED pages\n",
    "FinalJobLinks = np.unique(finallinks) #we take only unique job links\n",
    "\n",
    "print(FinalJobLinks.size, \"Jobs to download:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're on job number 0\n",
      "We're on job number 1\n",
      "We're on job number 2\n",
      "We're on job number 3\n",
      "We're on job number 4\n",
      "We're on job number 5\n",
      "We're on job number 6\n",
      "We're on job number 7\n",
      "We're on job number 8\n",
      "We're on job number 9\n",
      "We're on job number 10\n",
      "We're on job number 11\n",
      "We're on job number 12\n",
      "We're on job number 13\n",
      "We're on job number 14\n",
      "We're on job number 15\n",
      "We're on job number 16\n",
      "We're on job number 17\n",
      "We're on job number 18\n",
      "We're on job number 19\n",
      "We're on job number 20\n",
      "We're on job number 21\n",
      "We're on job number 22\n",
      "We're on job number 23\n",
      "We're on job number 24\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['Title','Description']) #create empty data fr\n",
    "for x in range(FinalJobLinks.size):\n",
    "        print(\"We're on job number %d\" %(x))\n",
    "        NAUKRIHealthCare = FinalJobLinks[x] #specify the job url\n",
    "        req2 = urllib.request.Request(NAUKRIHealthCare, headers=hdr)\n",
    "        page = urllib.request.urlopen(req2) #Query the website and return the html to the variable 'page'\n",
    "        soup = BeautifulSoup(page, \"html.parser\") #Parse the html in the 'page' variable, and store it in Beautiful Soup format\n",
    "        #print(NAUKRIHealthCare)\n",
    "        title = soup.h1.text  # get title\n",
    "\n",
    "\n",
    "        tmplist = re.split(r\"Job Type\", soup.select_one(\"p[class*=cl]\").text)\n",
    "        description = tmplist[0].replace(\"Job Summary\", '').replace(\"Job Description\", '') #get description\n",
    "        title = title.strip() #remove blank characters\n",
    "        title = title.encode('utf-8') #put utf8 encoding because of arabic and some other special characters\n",
    "        description = description.strip()\n",
    "        description = description.encode('utf-8')\n",
    "        df = df.append({\n",
    "             \"Title\": title,\n",
    "            \"Description\":  description}, ignore_index=True)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#print(df.loc[x])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('NaukriGulfhealthcare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
